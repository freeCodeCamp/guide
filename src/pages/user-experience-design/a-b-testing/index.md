---
title: A/B Testing
---
## A/B Testing

In A/B testing, also known as split-run testing, you have two versions of an element (A and B). To determine which version is better or has more impact, you subject both versions to experimentation simultaneously. In the end, you measure which version was more successful and select that version for real-world use.

As the name suggests, you are comparing two versions A and B, which are indentical except for one variation that may affect a user's behaviour.

Eg. Say you have a webpage and a sign-up button but you aren't sure about of color should the button be: green or blue? So you launch two versions wherein everything is the same except for the color of sign-up button. When the experiment / test ends, you see that the number of sign-ups increased when the button was blue, giving you a fair idea that blue button has higher impact on users' behaviour than the green one.

Similarly, the test may be extended to many other variables that affect users' behaviour such as background color, responsive design, orientation of data on the page, etc.

Hence, the goal of A/B testing is to identify changes to web pages that increase or maximize an outcome of interest.

![A/B Testing](https://i.imgur.com/NnUtQlS.gif)
