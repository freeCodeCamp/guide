webpackJsonp([0x5236c7be11e5e00],{"./node_modules/json-loader/index.js!./.cache/json/machine-learning-backpropagation.json":function(e,a){e.exports={data:{markdownRemark:{html:'<h2>Backpropagation</h2>\n<p>Backprogapation is a subtopic of <a href="../neural-networks/index.md">neural networks</a>, and is the process by which you calculate the gradients of each node in the network. These gradients measure the “error” each node contributes to the output layer, so in training a neural network, these gradients are minimized.</p>\n<p>Note: Backpropagation, and machine learning in general, required significant familiarity with linear algebra and matrix manipulation. Coursework or reading on this topic is highly recommended before trying to understand the contents of this article. </p>\n<h3>Computation</h3>\n<p>The process of backpropagation can be explained in three steps.</p>\n<p>Given the following</p>\n<ul>\n<li>m training examples (x,y) on a neural network of L layers</li>\n<li>g = the sigmoid function</li>\n<li>Theta(i) = the transition matrix from the ith to the i+1th layer</li>\n<li>a(l) = g(z(l)); an array of the values of the nodes in layer l based on one training example</li>\n<li>z(l) = Theta(l-1)a(l-1)</li>\n<li>Delta a set of L matricies representing transitions between the ith and i+1th layer</li>\n<li>d(l) = the array of the gradients for layer l for one training example</li>\n<li>D a set of L matricies with the final gradients for each node</li>\n<li>lambda the regualrization term for the network</li>\n</ul>\n<p>In this case, for matrix M, M’ will denote the transpose of matrix M</p>\n<ol>\n<li>\n<p>Assign all entries of the Delta(i), for i from 1 to L, zero.</p>\n</li>\n<li>\n<p>For each training example t from 1 to m, perform the following:</p>\n</li>\n<li>\n<p>perform forward propagation on each example to compute a(l) and z(l) for each layer</p>\n</li>\n<li>\n<p>compute d(L) = a(L) - y(t)</p>\n</li>\n<li>\n<p>compute d(l) = (Theta(l)’ • d(l+1)) • g(z(l)) for l from L-1 to 1</p>\n</li>\n<li>\n<p>increment Delta(l) by delta(l+1) • a(l)’</p>\n</li>\n<li>\n<p>Plug the Delta matricies into our partial derivative matricies\nD(l) = 1\\m(Delta(l) + lambda • Theta(l)); if l≠0\nD(l) = 1\\m • Delta(l); if l=0</p>\n</li>\n</ol>\n<p>Of course, just seeing this article looks hugely complicated and should really only be understood in the greater contexts of neural networks and machine learning. Please look at the arrached references for a better understanding of the topic as a whole.</p>\n<h4>More Information:</h4>\n<p>Lecture 4 CS231n <a href="https://youtu.be/d14TUNcbn1k?t=354">Introduction to Neural Networks</a>\nSiraj Raval - <a href="https://www.youtube.com/watch?v=q555kfIFUCM">Backpropagation in 5 Minutes</a>\n<a href="https://www.coursera.org/learn/machine-learning/">Andrew Ng’s ML Course</a>\n<a href="https://brilliant.org/wiki/backpropagation/">In depth, wiki style article</a>\n<a href="https://en.wikipedia.org/wiki/Backpropagation">Backprop on Wikipedia</a>\n<a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">A step-by-step example</a></p>',fields:{slug:"/machine-learning/backpropagation/"},frontmatter:{title:"Backpropagation"}}},pathContext:{slug:"/machine-learning/backpropagation/"}}}});
//# sourceMappingURL=path---machine-learning-backpropagation-7a35177bcc9e42a1b9ca.js.map