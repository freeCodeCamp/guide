---
title: Hadoop
---
## ![Hadoop](http://2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com/wp-content/uploads/2014/08/Hadoop_logo_2.png)

### Did you know? 
Hadoop logo is a yellow elephant. The name it self was coined by the son of Doug Cutting - One of the Founders of Hadoop and Chief Architect at Cloudera. When he was a child, playing with his toy elephant, he would say HADOOP. And Doug Cutting wrote it down thinking he might need it later because he was in software biz. And he later pulled it out when he was working with Hadoop. From there comes the name and the logo!

### What is Hadoop?

Hadoop is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.  It started with being on 5-6 computers.

In early 2000s Google released their paper on GFS (Google file system- They were able to store their data in large collection of commodity hardware) and Map reduce. They were propriety distributed file system and Doug wanted to create an open source distributed file system. And thus he and his colleague started working on Apache Nutch. which was later Hadoop.

### Why do we need Hadoop?

According to IBM: "Every day, 2.5 billion gigabytes of high-velocity data are created in a variety of forms, such as social media posts, information gathered in sensors and medical devices, videos and transaction records"

Examples of data:
  * Phone data
  * Website logs
  * Xrays data

What is big data?
Big data is a subjective term. While most people would consider a data size of several terrabytes to be big. However, big data can be data that cannot be produced on a single machine.

Challenges with big data:
- Size
- Speed with which data is created
- Data comes from various sources in various formats

Known as the 3Vs:
- Volumes
- Velocity
- Variety

To deal with big data, we need a robust system, Hadoop.

### Core Hadoop

The big data is stored in HDFS (Hadoop Distributed file system). HDFS is typically clusters of usually different rack-mounted servers. This big data can be later processed by using MAP REDUCE algorithm(When you have a very large file, to parse the entire file line by line may take a long time. Map Reduce(MR) divides the file into chunks and processes it in parallel). 

In Hadoop 1.x, MR included a Job Tracker and Task Trackers to manage the cluster resources. The Job Tracker determined what resources were available to run MR jobs and which nodes to use. In Hadoop 2.x, Yet Another Resource Negotiator (YARN) was introduced with Map Reduce 2. YARN not only improved core Hadoop cluster resource management; it also allowed other big data components in the Hadoop ecosystem (HBase, Hive, Spark, etc) to utilize the same cluster's resources.

### Hadoop Ecosystem

Lot of software has grown around Hadoop and Map Reduce. This collectively is known as the Hadoop Ecosystem. Some softwares are intended to make it easier to load the data into the Hadoop cluster and some are intended to make it easier to use. Eg. Business intelligence softwares wanted to hook to Hadoop ecosystem to be able to query data using SQL queries and not use programming languages such as JAVA, python, ruby etc as is needed for map reduce. Hive and pig are two such scripting languages that allow users to code using simple language. This code is converted to Map Reduce and then run on the Hadoop system.

Another software that is used to access the data is IMPALA. This does not use Map Reduce and is built on Hadoop to optimized for low latency queries. It runs a lot faster Hive (many times faster). Hive is optimized for running long batch processing jobs.

Another project used is Sqoop which takes data from traditional relational database and puts in Hadoop so that it can be processed with other data already present in the Hadoop file system. Flume is a project that ingests data as it is generated by external systems. HBase is a real time database built on top of HDFS. Hue is a graphical front end system. Oozie is a workflow management tool. Mahout is a machine learning library.

Cloudera, Horton works, MapR, etc are companies that glue these together and test whether the components work well with each other. Also makes it easier for us to install.

### Hadoop Distributed File System

Let us assume you want to store a file store.txt which is of size 100 MB. It can be divided into chunks of 64 MB and 36 MB called blocks - blk_1, blk_2. As the file is uploaded it will get uploaded into nodes in the cluster. A piece of software called Damon runs on each node in the cluster, called DataNode. There is a separate machine that runs another piece of Damon called NameNode that helps identify which blocks go together and which DataNode they reside on. Information stored on the NameNode is called metadata.

There are some problems associated with this design:
1. If there is a network failure
1. If there is a down time in one of the datanodes
1. If there is a down time in the namenode

To fix this issue, hadoop keeps redundant data. 3 copies of each block is kept in different datanodes. This helps in fixing problem 1 and 2 but not the 3rd.

To fix the 3rd problem, concept called NameNode standby. There are 2 namenodes used here. One is active, one is standby with the standby taking over when the active namenode undergoes failure.

#### More Information:
1. <a href='https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617' target='_blank' rel='nofollow'>Udacity course on hadoop</a>
1. <a href='http://hadoop.apache.org/' target='_blank' rel='nofollow'>Apache Hadoop</a>
